# -*- coding: utf-8 -*-
"""createSavedDataset

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZacpSiFGSqV9Y4E_jTbwWof5AUSls-og
"""

import tensorflow as tf
import customImputerLayerDefinition as myImputer

# Define the feature description
feature_description = {
    'tickers': tf.io.FixedLenFeature([188], tf.float32, np.zeros(188)),
    'weekday': tf.io.FixedLenFeature([], tf.int64, default_value=0),
    'month': tf.io.FixedLenFeature([], tf.int64, default_value=0),
    'hour': tf.io.FixedLenFeature([], tf.int64, default_value=0),
    'target': tf.io.FixedLenFeature([], tf.int64, default_value=0)
}

# Parse the serialized examples into a dictionary of tensors
def parse_example(serialized_example):
    example = tf.io.parse_example(serialized_example, feature_description)
    features = {k: v for k, v in example.items() if k != 'target'}
    target = example['target']
    return features, target

# Load the TFRecord dataset
dataset = tf.data.TFRecordDataset(['dataset.tfrecord']).batch(32).map(parse_example).cache()

# Get the total number of examples in the dataset
dataset_size = dataset.cardinality().numpy() 

# Split the dataset into training, validation, and testing datasets
train = dataset.take(int(0.7 * dataset_size))
remaining = dataset.skip(int(0.7 * dataset_size))
validation = remaining.take(int(0.15 * dataset_size))
test = remaining.skip(int(0.15 * dataset_size))

# Define input layers
inputDict = {
    'tickers': tf.keras.Input(shape=(188,), dtype=tf.float32),
    'weekday': tf.keras.Input(shape=(), dtype=tf.int64),
    'month': tf.keras.Input(shape=(), dtype=tf.int64),
    'hour': tf.keras.Input(shape=(), dtype=tf.int64)
}

tickers_input = tf.concat(list(train.map(lambda x,y: x['tickers'])), axis=0)
# Create an instance of the Imputer layer
imputer = myImputer()
imputer.adapt(tickers_input)

# Apply the Imputer layer to the tickers input
imputed_tickers = imputer(tickers_input)

# Create a Normalization layer and adapt it to the output of the Imputer
normalizer = tf.keras.layers.experimental.preprocessing.Normalization()
normalizer.adapt(imputed_tickers)
normalized_tickers = normalizer(imputed_tickers)

# # Update inputDict with the processed tickers
# inputDict['tickers'] = normalized_tickers

weekday_embedding = tf.keras.layers.Embedding(6, 2)(inputDict['weekday'])
month_embedding = tf.keras.layers.Embedding(12, 2)(inputDict['month'])
hour_embedding = tf.keras.layers.Embedding(24, 2)(inputDict['hour'])

# Flatten the embedding outputs
weekday_flattened = tf.keras.layers.Flatten()(weekday_embedding)
month_flattened = tf.keras.layers.Flatten()(month_embedding)
hour_flattened = tf.keras.layers.Flatten()(hour_embedding)

# Concatenate all the inputs
preproced = tf.keras.layers.Concatenate()([normalized_tickers, weekday_flattened, month_flattened, hour_flattened])

restMod = tf.keras.Sequential([tf.keras.layers.Dense(128,activation='relu'),
                               tf.keras.layers.Dense(23, activation='softmax')])

decs = restMod(preproced)
whole_model = tf.keras.Model(inputs=inputDict, outputs=decs)

whole_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
whole_model.summary()

history = whole_model.fit(train, epochs=10, validation_data=validation)